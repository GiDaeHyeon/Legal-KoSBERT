import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport pytorch_lightning as plfrom .model import KoSBERTclass KoSBertTrainModule(pl.LightningModule):    def __init__(self,                 sbert: nn.Module = KoSBERT(),                 learning_rate: float = 5e-4,                 objective_function: str = None,                 ):        super(KoSBertTrainModule, self).__init__()        self.sbert = sbert        self.learning_rate = learning_rate        if objective_function is None:            print('목적함수를 설정해주십쇼.')            raise NotImplementedError        elif objective_function == 'classification':            self.loss_fn = nn.NLLLoss()            self.W_t = nn.Linear(768 * 3, 3)        elif objective_function == 'regression':            self.loss_fn = nn.MSELoss()        elif objective_function == 'triplet':            self.loss_fn = nn.TripletMarginLoss()        self.objective_function = objective_function    def configure_optimizers(self):        optimizer = optim.AdamW(lr=self.learning_rate)        return [optimizer]    def forward(self,                input_ids: torch.Tensor,                attention_mask: torch.Tensor,                token_type_ids: torch.Tensor) -> torch.Tensor:        return self.sbert(input_ids=input_ids,                          attention_mask=attention_mask,                          token_type_ids=token_type_ids)    def train_embedding(self, data: dict) -> torch.Tensor:        if self.objective_function == 'classification':            sent1, sent2, y = data            logit_sent1 = self(input_ids=sent1.input_ids,                               attentation_mask=sent1.attentation_mask,                               token_type_ids=sent1.token_type_ids)            logit_sent2 = self(input_ids=sent2.input_ids,                               attentation_mask=sent2.attentation_mask,                               token_type_ids=sent2.token_type_ids)            logits = self.W_t(torch.cat([logit_sent1, logit_sent2, torch.sub(logit_sent1, logit_sent2)], 1))            y_hat = F.log_softmax(logits)            loss = self.loss_fn(y, y_hat)        elif self.objective_function == 'regression':            sent1, sent2, y = data            logit_sent1 = self(input_ids=sent1.input_ids,                               attentation_mask=sent1.attentation_mask,                               token_type_ids=sent1.token_type_ids)            logit_sent2 = self(input_ids=sent2.input_ids,                               attentation_mask=sent2.attentation_mask,                               token_type_ids=sent2.token_type_ids)            y_hat = F.cosine_similarity(logit_sent1, logit_sent2)            loss = self.loss_fn(y, y_hat)        elif self.objective_function == 'triplet':            texts, _ = data            anchor, positive, negative = texts            emb_anchor = self(input_ids=anchor.input_ids,                              attentation_mask=anchor.attentation_mask,                              token_type_ids=anchor.token_type_ids)            emb_positive = self(input_ids=positive.input_ids,                                attentation_mask=positive.attentation_mask,                                token_type_ids=positive.token_type_ids)            emb_negative = self(input_ids=negative.input_ids,                                attentation_mask=negative.attentation_mask,                                token_type_ids=negative.token_type_ids)            loss = self.loss_fn(emb_anchor, emb_positive, emb_negative)        return loss    def training_step(self, data: dict) -> dict:        loss = self.train_embedding(data)        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)        return {'loss': loss}    def validation_step(self, data: dict) -> dict:        loss = self.train_embedding(data)        self.log('val_loss', loss, prog_bar=False, on_step=False, on_epoch=True)        return {'loss': loss}