import torchfrom torch.utils.data import Dataset, DataLoaderfrom transformers import BertTokenizerFastimport pytorch_lightning as plclass KorNLIDataset(Dataset):    def __init__(self,                 data_dir: str = None,                 tokenizer_weight: str = 'kykim/bert-kor-base',                 max_length: int = None):        super(KorNLIDataset, self).__init__()        # TODO 데이터셋 정의    def __len__(self):        return None    def __getitem__(self, idx):        return Noneclass KorSTSDataset(Dataset):    def __init__(self,                 train: bool,                 data_dir: str = None,                 tokenizer_weight: str = 'kykim/bert-kor-base',                 max_length: int = None):        super(KorSTSDataset, self).__init__()        phrase = 'test' if train else 'train'        with open(f'{data_dir}/sts-{phrase}.tsv', 'r') as f:            self.raw_data = f.readlines()        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_weight)        self.max_length = max_length    def __len__(self):        return len(self.raw_data)    def __getitem__(self, idx):        sent1_raw, sent2_raw, y = self.raw_data[idx].split('\t')[5], \                                  self.raw_data[idx].split('\t')[6][:-1], \                                  self.raw_data[idx].split('\t')[4]        sent1 = self.tokenizer.encode_plus(sent1_raw,                                           padding='max_length',                                           max_length=self.max_length)        for k in sent1:            sent1[k] = torch.Tensor(sent1[k])        sent2 = self.tokenizer.encode_plus(sent2_raw,                                           padding='max_length',                                           max_length=self.max_length)        for k in sent2:            sent2[k] = torch.Tensor(sent2[k])        return sent1, sent2, torch.Tensor(float(y))class KorSNLIDataset(Dataset):    def __init__(self,                 data_dir: str = None,                 tokenizer_weight: str = 'kykim/bert-kor-base',                 max_length: int = None):        super(KorSNLIDataset, self).__init__()        # TODO 데이터셋 정의    def __len__(self):        return None    def __getitem__(self, idx):        return Noneclass KoSBertDataModule(pl.LightningDataModule):    def __init__(self,                 which_dataset: str = None,                 data_dir: str = None,                 tokenizer_weight: str = 'kykim/bert-kor-base',                 max_length: int = None,                 batch_size: int = 32,                 num_workers: int = 4):        super(KoSBertDataModule, self).__init__()        if which_dataset == 'STS':            self.train_dataset = KorSTSDataset(train=True,                                               data_dir=data_dir,                                               tokenizer_weight=tokenizer_weight,                                               max_length=max_length)            self.val_dataset = KorSTSDataset(train=False,                                             data_dir=data_dir,                                             tokenizer_weight=tokenizer_weight,                                             max_length=max_length)            self.batch_size = batch_size            self.num_workers = num_workers    def train_dataloader(self) -> DataLoader:        return DataLoader(dataset=self.train_dataset,                          batch_size=self.batch_size,                          num_workers=self.num_workers,                          shuffle=True,                          pin_memory=True,                          drop_last=True)    def val_dataloader(self) -> DataLoader:        return DataLoader(dataset=self.val_dataset,                          batch_size=self.batch_size * 2,                          num_workers=self.num_workers,                          shuffle=True,                          pin_memory=True,                          drop_last=True)